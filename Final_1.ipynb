{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bd0821ab-a365-4d51-84bc-1601aac1db3a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\sumit\\Desktop\\project\\pyspark\\NLP\\Final_1.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sumit/Desktop/project/pyspark/NLP/Final_1.ipynb#W0sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#Creating a spark Session\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/sumit/Desktop/project/pyspark/NLP/Final_1.ipynb#W0sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkSession\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sumit/Desktop/project/pyspark/NLP/Final_1.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m spark \u001b[39m=\u001b[39m SparkSession \\\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sumit/Desktop/project/pyspark/NLP/Final_1.ipynb#W0sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         \u001b[39m.\u001b[39mbuilder \\\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sumit/Desktop/project/pyspark/NLP/Final_1.ipynb#W0sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         \u001b[39m.\u001b[39mappName(\u001b[39m'\u001b[39m\u001b[39mTwitter_analysis\u001b[39m\u001b[39m'\u001b[39m) \\\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sumit/Desktop/project/pyspark/NLP/Final_1.ipynb#W0sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         \u001b[39m.\u001b[39mgetOrCreate()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sumit/Desktop/project/pyspark/NLP/Final_1.ipynb#W0sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mSession created\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "#Creating a spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName('Twitter_analysis') \\\n",
    "        .getOrCreate()\n",
    "print('Session created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mounting s3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mount_s3_bucket(access_key, secret_key, bucket_name, mount_folder):\n",
    "  ACCESS_KEY_ID = access_key\n",
    "  SECRET_ACCESS_KEY = secret_key\n",
    "  ENCODED_SECRET_KEY = SECRET_ACCESS_KEY.replace(\"/\", \"%2F\")\n",
    "\n",
    "  print (\"Mounting\", bucket_name)\n",
    "\n",
    "  try:\n",
    "    # Unmount the data in case it was already mounted.\n",
    "    dbutils.fs.unmount(\"/mnt/%s\" % mount_folder)\n",
    "    \n",
    "  except:\n",
    "    # If it fails to unmount it most likely wasn't mounted in the first place\n",
    "    print (\"Directory not unmounted: \", mount_folder)\n",
    "    \n",
    "  finally:\n",
    "    # Lastly, mount our bucket.\n",
    "    dbutils.fs.mount(\"s3a://%s:%s@%s\" % (ACCESS_KEY_ID, ENCODED_SECRET_KEY, bucket_name), \"/mnt/%s\" % mount_folder)\n",
    "    #dbutils.fs.mount(\"s3a://\"+ ACCESS_KEY_ID + \":\" + ENCODED_SECRET_KEY + \"@\" + bucket_name, mount_folder)\n",
    "    print (\"The bucket\", bucket_name, \"was mounted to\", mount_folder, \"\\n\")\n",
    "    \n",
    "access_key = \n",
    "secret_key = \n",
    "mount_s3_bucket(access_key, secret_key, 'your/destimation','name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "af0d5240-32f9-4738-a332-9bf629fbeed9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Reading the tweets\n",
    "tweets = spark.read.option('header',False).csv('/mnt/KingofPop/*/*/*/*/')\n",
    "display(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2a291eaa-07e7-4b68-8012-d2e0c08bc783",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Moving it to the cache and getting the count\n",
    "tweets.cache()\n",
    "tweets.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5c7b213d-213a-4a33-81fe-d02e50699954",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Renaming columns with spark functions\n",
    "from pyspark.sql.functions import regexp_extract, col\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "tweets = tweets.select(*[F.col('_c0').alias('tweet'), F.col(\"_c1\").alias('comments')])\n",
    "display(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ee6172a2-a14e-478f-a3f2-932a8e12607f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Cleaning Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7c7ea3da-6f4e-470a-bf8d-457f55067c2d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Extracting time using regex \n",
    "tweets = tweets.withColumn('Time', regexp_extract(col('tweet'), '(.)Aug\\s+\\w+.*',0))\\\n",
    "                .withColumn('RT', regexp_extract(col('tweet'), '\\s(RT)',0))\\\n",
    "                .withColumn('Mentions', regexp_extract(col('tweet'),'\\s(@)([^\\s]+)',0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "93b0f316-fc85-4f44-b423-8faa62189543",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Removing symbols and number and Dates.\n",
    "tweets_clean1 = tweets.withColumn('tweet', F.regexp_replace('tweet', r\"http\\S+\", \"\")) \\\n",
    "                    .withColumn('tweet', F.regexp_replace('tweet', r\"[^a-zA-z]\", \" \")) \\\n",
    "                    .withColumn('tweet', F.regexp_replace('tweet', r\"\\s+\", \" \")) \\\n",
    "                    .withColumn('tweet', F.lower('tweet')) \\\n",
    "                    .withColumn('tweet', F.trim('tweet'))\\\n",
    "                    .withColumn('tweet', F.regexp_replace('tweet', r\"(none)\\s(.*)\",\"\"))\n",
    "display(tweets_clean1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a23bdd77-c33f-440f-a4e8-6461f860bec8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d03c50d8-6d89-4c3f-95ef-6be87119fb23",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Installing textblob for sentiment\n",
    "#pip install textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a7acf54f-41af-433b-96a0-694d81360345",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Feature Eng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2ccb8450-4817-4021-b4ee-3e87cdaf929d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from pyspark.sql.types import FloatType,StringType\n",
    "def sentiments(x):\n",
    "    testimonial = TextBlob(x)\n",
    "    \"\"\" Getting the score and classifiying it into 2 categories \"\"\"\n",
    "    polarity_score = testimonial.sentiment.polarity\n",
    "    if polarity_score <= 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "#Creating a function and registering it \n",
    "spark.udf.register('sentiments',sentiments,StringType())\n",
    "function_def = udf(lambda z: sentiments(z),StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8328b5ae-d385-4b64-9f44-9df0bf17750e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Running the function to add sentiments to the corresponding column\n",
    "tweets_clean = tweets_clean1.withColumn( 'sentiment',function_def('tweet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8a81ddf6-3a8c-47af-a908-e48004342175",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(tweets_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2c4ae61c-e7ef-4bc0-a391-463fb473016f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "ML pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "be3bae5e-9732-4bf0-9302-468ab3fc0f2c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram, VectorAssembler, StopWordsRemover, HashingTF, IDF, Tokenizer, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Use 90% cases for training, 10% cases for testing\n",
    "train, test = tweets_clean.randomSplit([0.7, 0.3], seed=20200819)\n",
    "\n",
    "# Create transformers for the ML pipeline\n",
    "tokenizer = Tokenizer(inputCol=\"tweet\", outputCol=\"tokens\")\n",
    "\n",
    "#Removng stopworkd \"the , and etc..\"\n",
    "stopword_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\")\n",
    "\n",
    "# Converting each word to a vecotr (similarly like one hot encoding)\n",
    "cv = CountVectorizer(vocabSize=2**16, inputCol=\"filtered\", outputCol='cv')\n",
    "\n",
    "#How common the words are used throughout the tweet database\n",
    "idf = IDF(inputCol='cv', outputCol=\"1gram_idf\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "\n",
    "#Making all the items into a single vector\n",
    "assembler = VectorAssembler(inputCols=[\"1gram_idf\"], outputCol=\"features\")\n",
    "\n",
    "#using StringIndexer for assiging label values as integer \n",
    "label_encoder= StringIndexer(inputCol = \"sentiment\", outputCol = \"label\")\n",
    "\n",
    "#using logistics regression for ml modeling\n",
    "lr = LogisticRegression(maxIter=100)\n",
    "\n",
    "#Creating a pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, stopword_remover, cv, idf, assembler, label_encoder, lr])\n",
    "\n",
    "pipeline_model = pipeline.fit(train)\n",
    "predictions = pipeline_model.transform(test)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(test.count())\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Accuracy Score: {0:.4f}\".format(accuracy))\n",
    "print(\"ROC-AUC: {0:.4f}\".format(roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "44e10250-1459-4216-b19f-4395d93a1958",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predictions = pipeline_model.transform(tweets_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8bab2ad0-2621-402a-920c-f0429cd0bc2c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(predicitons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "59f5eddc-bace-4ae7-bea3-c2d6b6bf4336",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predictions2 = predicitons.select('tweet','Time','prediction')\n",
    "predictions2 = predictions2.withColumn('Time',predictions2.Time.cast('Date'))\n",
    "predictions2.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b09c9641-5025-4f71-bfed-159040fd48bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(predictions2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f9165da4-9695-41cb-8b4b-7c7d10ab4f9e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(predicitons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bd1148bf-0208-414c-80f2-a4910f4e9aae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "predictions2 = (predictions2.withColumn('prediction',predictions2.prediction.cast('Double')))\n",
    "predictions2.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ccb39456-8a3c-4720-9da6-94d7dff4bfe6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Final_1",
   "notebookOrigID": 455935280338892,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "a55c7acd63d93e16d9379f9829e45b74a7168d1ec69ec3c1e50746408e0e84a1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
